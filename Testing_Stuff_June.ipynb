{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('PHD': conda)"
  },
  "interpreter": {
   "hash": "eaa00e46caca58319536d0032152cd5eb76a9b72fbc9a3af026d9e62232c8082"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = input('Suffix: ')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, MinMaxScaler, StandardScaler, LabelBinarizer\n",
    "from sklearn.preprocessing import normalize, robust_scale, minmax_scale\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,KBinsDiscretizer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, GroupKFold, KFold, train_test_split, LeaveOneOut\n",
    "\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, roc_auc_score, roc_curve, plot_confusion_matrix, f1_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pymc3 as pm\n",
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "import theano\n",
    "\n",
    "from Preprocessing_Methods import *\n",
    "from TSquared.hotelling_t2 import HotellingT2\n",
    "\n",
    "total = pd.read_pickle('./Original_Data.pickle')\n",
    "#total = pd.read_hdf('/mnt/c/Users/conor/Git_Projects/PHD/Tumour_df_raw_0804.hdf5', key='Data')\n",
    "train_indices = pd.read_pickle('./train_indices.pickle')\n",
    "#total = pd.read_hdf('/condor_data/sgcwhitl/Bayesian/Datasets/Original.hdf', key='Data')\n",
    "totali = truncate(total, start=1000, end=1800)\n",
    "\n",
    "ht2 = HotellingT2().fit(totali)\n",
    "total = total.loc[ht2.predict(totali)==1,:]\n",
    "\n",
    "patient_ids = total.reset_index()['Patient_nu '].unique()\n",
    "\n",
    "total['1yeardeath'] = (total.reset_index()['survival (months)']<12).values & (total.reset_index('DiedvsAlive')['DiedvsAlive']=='Died').values\n",
    "\n",
    "total = total.set_index('1yeardeath', append=True)\n",
    "\n",
    "y = '1yeardeath'\n",
    "npca = 20\n",
    "\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "(\"Normalise spectra\", FunctionTransformer(minmax_scale, kw_args = {\"axis\": 1})),\n",
    "(\"Scaler\", RobustScaler()),\n",
    "(\"PCA\", PCA(npca)),\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"OneHot\", OneHotEncoder())\n",
    "])\n",
    "\n",
    "#in_df = total.reset_index('ASMA').dropna(subset=['ASMA']).sample(20000)\n",
    "in_df = total#.sample(20000)\n",
    "in_df.columns = [str(col) for col in in_df.columns]\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (numeric_pipe,     make_column_selector(dtype_include=np.number)),\n",
    "    (categorical_pipe, make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "bootstrap_n = 0\n",
    "total_data = []\n",
    "traces = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-604903cff90f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0min_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Patient_Number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Patient_Number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/PHD/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "in_df.reset_index('Patient_Number')[['Patient_Number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 0\n",
    "splits = []\n",
    "\n",
    "while n_splits < 100:\n",
    "\n",
    "    train_pat, test_pat = train_test_split(patient_ids, test_size=0.1)\n",
    "\n",
    "    train_data = in_df.query(f\"Patient_Number in {list(train_pat)}\")\n",
    "    test_data = in_df.query(f\"Patient_Number in {list(test_pat)}\")\n",
    "\n",
    "    if len(np.unique(train_data.reset_index(y)[y])) == len(np.unique(test_data.reset_index(y)[y])) == 2:\n",
    "\n",
    "        splits.append({'Train_pats': train_pat,\n",
    "                       'Test_pats':  test_pat})\n",
    "\n",
    "        n_splits += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(splits).to_pickle('./train_indices_10_H.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, row in train_indices.iloc[:1,:].iterrows():\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    train_data = in_df.query(f\"Patient_Number in {list(row['Train_pats'])}\")\n",
    "    test_data = in_df.query(f\"Patient_Number in {list(row['Test_pats'])}\")\n",
    "\n",
    "    #columns = np.concatenate([[f'PCA{i}' for i in range(1,npca+1)], [f'ASMA:{t}' for t in train_data['ASMA'].unique()]])\n",
    "    columns = [f'PCA{i}' for i in range(1,npca+1)]\n",
    "\n",
    "    # Transform FTIR data to PCA components\n",
    "    X_train = pd.DataFrame(ct.fit_transform(train_data), columns=columns, index=train_data.index)\n",
    "    X_test = pd.DataFrame(ct.transform(test_data), columns=columns, index=test_data.index)\n",
    "\n",
    "    Y_train = train_data.index.get_level_values(y).astype(np.int)\n",
    "    Y_test  = test_data.index.get_level_values(y).astype(np.int)\n",
    "\n",
    "\n",
    "    ################################ Bayesian ################################\n",
    "\n",
    "    ncat = 3\n",
    "    ncon = npca\n",
    "\n",
    "    with pm.Model() as logistic_model:\n",
    "\n",
    "        data_ = pm.Data('Pred', X_train.T)\n",
    "        obs = pm.Data('Observed', Y_train.values.T)\n",
    "\n",
    "        ɛ = pm.HalfNormal('ɛ', sd=100)\n",
    "    \n",
    "        # Continuous variables for each PC\n",
    "        β1 = pm.Normal(\"β1\", mu=0, sigma=10, shape = (ncon+1,1))\n",
    "\n",
    "        # β.T + ɛ\n",
    "        z = pm.math.dot(β1[1:].T, data_)\n",
    "\n",
    "        # Probability of parameter P given the data\n",
    "        p = pm.Deterministic('P', pm.math.sigmoid(z + (β1[0] + ɛ)))\n",
    "        observed = pm.Bernoulli(\"p\", p, observed=obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        #start=pm.find_MAP()\n",
    "\n",
    "        callback = CheckParametersConvergence(diff='absolute')\n",
    "        approx = pm.fit(n=50000, callbacks=[callback])\n",
    "\n",
    "    with logistic_model:\n",
    "\n",
    "        trace = approx.sample(10000)\n",
    "\n",
    "        # update values of predictors:\n",
    "        pm.set_data({\"Pred\": X_test.T})\n",
    "        # use the updated values and predict outcomes and probabilities:\n",
    "        posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[\"P\"], samples=1000)\n",
    "\n",
    "\n",
    "    model_preds = posterior_predictive[\"P\"].squeeze()\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, Y_train)\n",
    "\n",
    "    results = {'y_test': Y_test,\n",
    "               'BLR_Posterior': pd.DataFrame(model_preds.T, index=test_data.index),\n",
    "               'LR_Preds': pd.DataFrame(lr.predict_proba(X_test), index=test_data.index)}\n",
    "\n",
    "    total_data.append(results)\n",
    "    traces.append(posterior_predictive)\n",
    "\n",
    "\n",
    "with open(f'./results/New/{y}_2206_{suffix}.pickle', 'wb') as f:\n",
    "\n",
    "    output_dict = {'Predictions': total_data,\n",
    "                   'Trace': traces}\n",
    "\n",
    "    pickle.dump(output_dict, f)#\n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ]
}