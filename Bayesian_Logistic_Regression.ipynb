{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('PHD': conda)",
   "metadata": {
    "interpreter": {
     "hash": "eaa00e46caca58319536d0032152cd5eb76a9b72fbc9a3af026d9e62232c8082"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import Cross_Eval\n",
    "from Preprocessing_Methods import *\n",
    "from stratgroupkfold import StratifiedGroupKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, MinMaxScaler, StandardScaler, LabelBinarizer\n",
    "from sklearn.preprocessing import normalize, robust_scale, minmax_scale\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, GroupKFold, KFold, train_test_split\n",
    "\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, roc_auc_score, roc_curve, plot_confusion_matrix, f1_score, recall_score, accuracy_score\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.utils import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from mlxtend.evaluate import scoring\n",
    "from mlxtend.evaluate import BootstrapOutOfBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Metadata/Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = pd.DataFrame(glob.glob(\"/mnt/c/Users/conor/Google Drive/PhD/Project with Janet/Extracted_Spectra/*.pickle\"), columns = [\"hdf_loc\"])\n",
    "\n",
    "try:all_files = pd.DataFrame(glob.glob(\"/mnt/d/Conor/Extracted_Spectra/*.pickle\"), columns = [\"hdf_loc\"])\n",
    "except:all_files = pd.DataFrame(glob.glob(\"/mnt/c/Users/conor/Google Drive/PhD/Project with Janet/Extracted_Spectra/*.pickle\"), columns = [\"hdf_loc\"])\n",
    "\n",
    "all_files = pd.DataFrame(glob.glob(\"/mnt/c/Users/conor/Google Drive/PhD/Project with Janet/Extracted_Spectra/*.pickle\"), columns = [\"hdf_loc\"])\n",
    "\n",
    "all_files[\"TMA\"] = all_files.apply(lambda row: float(row[\"hdf_loc\"].split(\"/\")[-1][4:5]), axis = 1)\n",
    "all_files[\"Core\"] = all_files.apply(lambda row: row[\"hdf_loc\"].split(\"/\")[-1][8:-7], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(79, 18)\n"
     ]
    }
   ],
   "source": [
    "try:metadata = pd.read_excel(\"/mnt/b/Google_Drive/PhD/Project with Janet/Scoring grids Advancing Front (TMAs 1-3).xlsx\", sheet_name = \"reformatted\")\n",
    "except:metadata = pd.read_excel(\"/mnt/c/Users/conor/Google Drive/PhD/Project with Janet/Scoring grids Advancing Front (TMAs 1-3).xlsx\", sheet_name = \"reformatted\")\n",
    "\n",
    "metadata = metadata.merge(all_files, left_on = [\"TMA\", \"TMA site\"], right_on = [\"TMA\", \"Core\"])\n",
    "print(metadata.shape)\n",
    "\n",
    "try:new_metadata = pd.read_excel(\"/mnt/b/Google_Drive/PhD/Project with Janet/new_metadata.xlsx\")\n",
    "except:new_metadata = pd.read_excel(\"/mnt/c/Users/conor/Google Drive/PhD/Project with Janet/new_metadata.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata[\"DODvsAlive\"] = new_metadata[\"OUTCOME\"].map(\n",
    "                            {\"DOD\" : \"DOD\",\n",
    "                             \"alive\": \"Other\",\n",
    "                             \"died other\" : \"Other\",\n",
    "                             \"died\" : \"Other\",\n",
    "                             \"Died\" : \"Other\",\n",
    "                             \"Died other\": \"Other\"})\n",
    "\n",
    "new_metadata[\"DiedvsAlive\"] = new_metadata[\"OUTCOME\"].map(\n",
    "                            {\"DOD\" : \"Died\",\n",
    "                             \"alive\": \"Alive\",\n",
    "                             \"died other\" : \"Died\",\n",
    "                             \"died\" : \"Died\",\n",
    "                             \"Died\" : \"Died\",\n",
    "                             \"Died other\": \"Died\"})\n",
    "\n",
    "new_metadata[\"ECS\"] = new_metadata[\"ECS\"].map(\n",
    "                            {\"y\" : \"Y\",\n",
    "                             \"Y\": \"Y\",\n",
    "                             \"N\" : \"N\",\n",
    "                             \"n\" : \"N\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata['lifespan'] = new_metadata['Date of death'] - new_metadata['date of diag']\n",
    "new_metadata[\"Years\"] = new_metadata['Date of death'].dt.year - new_metadata['date of diag'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did the patient survive beyond n years?\n",
    "for i in np.arange(0,5,1):\n",
    "    \n",
    "    s = (365*i)\n",
    "    e = (365*(i+2))\n",
    "    \n",
    "    new_metadata[f\"{i+1}year\"] = new_metadata.apply(lambda row: True if s<(row['lifespan']).days<e and row.OUTCOME not in ['Alive',\"alive\"] else False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata[f\"1yeardeath\"] = new_metadata.apply(lambda row: True if (row['lifespan']).days<365 and row.OUTCOME not in ['Alive',\"alive\"] else False, axis = 1)\n",
    "\n",
    "bins = np.arange(0, new_metadata[\"Years\"].max(),2)\n",
    "\n",
    "new_metadata[\"2years\"] = np.digitize(new_metadata[\"Years\"], bins)"
   ]
  },
  {
   "source": [
    "# Import ASMA data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:asma = pd.read_excel(\"/mnt/b/Google_Drive/PhD/Project with Janet/ASMA.xlsx\", sheet_name = \"Sheet3\")\n",
    "except:asma = pd.read_excel(\"/mnt/c/Users/conor/Google Drive/PhD/Project with Janet/ASMA.xlsx\", sheet_name = \"Sheet3\")\n",
    "\n",
    "new_metadata = new_metadata.merge(asma, left_on = \"Patient_Number\", right_on = \"Case ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Patient_Number gender date of diag  age @ diag            site pT  pN  \\\n",
       "0             3143      m   2004-03-29          58  floor of mouth  4   0   \n",
       "1             3167      m   2004-09-30          74  floor of mouth  2   2   \n",
       "2             3169      m   2004-10-28          63  floor of mouth  3   1   \n",
       "3             3170      m   2004-11-22          59           other  4  2b   \n",
       "4             3230      m   2006-01-26          74          tongue  2   1   \n",
       "..             ...    ...          ...         ...             ... ..  ..   \n",
       "97            3568      m   2010-03-04          74  floor of mouth  2  2c   \n",
       "98            3574      m   2010-04-15          51          tongue  2  2b   \n",
       "99            3578      f   2010-04-26          44           other  4  2b   \n",
       "100           3583      m   2010-06-07          56          tongue  3  2b   \n",
       "101           3585      m   2010-05-13          68          tongue  2   1   \n",
       "\n",
       "    stage ECS     OUTCOME  ... Years  1year  2year  3year  4year  5year  \\\n",
       "0       4   N         DOD  ...   9.0  False  False  False  False  False   \n",
       "1       4   Y         DOD  ...   1.0   True   True  False  False  False   \n",
       "2       3   N  died other  ...   2.0   True   True  False  False  False   \n",
       "3      4a   Y  Died other  ...  10.0  False  False  False  False  False   \n",
       "4       3   Y         DOD  ...   0.0   True  False  False  False  False   \n",
       "..    ...  ..         ...  ...   ...    ...    ...    ...    ...    ...   \n",
       "97     4a   Y  died other  ...   0.0   True  False  False  False  False   \n",
       "98     4a   N  died other  ...   5.0  False  False  False  False   True   \n",
       "99    NaN   Y       alive  ...   NaN  False  False  False  False  False   \n",
       "100    4a   Y       alive  ...   NaN  False  False  False  False  False   \n",
       "101     3   N       alive  ...   NaN  False  False  False  False  False   \n",
       "\n",
       "    1yeardeath 2years Case ID ASMA  \n",
       "0        False      5    3143    H  \n",
       "1        False      1    3167    H  \n",
       "2        False      2    3169    L  \n",
       "3        False      6    3170    H  \n",
       "4         True      1    3230    H  \n",
       "..         ...    ...     ...  ...  \n",
       "97        True      1    3568    H  \n",
       "98       False      3    3574    I  \n",
       "99       False      6    3578  NaN  \n",
       "100      False      6    3583    L  \n",
       "101      False      6    3585    H  \n",
       "\n",
       "[102 rows x 30 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Number</th>\n      <th>gender</th>\n      <th>date of diag</th>\n      <th>age @ diag</th>\n      <th>site</th>\n      <th>pT</th>\n      <th>pN</th>\n      <th>stage</th>\n      <th>ECS</th>\n      <th>OUTCOME</th>\n      <th>...</th>\n      <th>Years</th>\n      <th>1year</th>\n      <th>2year</th>\n      <th>3year</th>\n      <th>4year</th>\n      <th>5year</th>\n      <th>1yeardeath</th>\n      <th>2years</th>\n      <th>Case ID</th>\n      <th>ASMA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3143</td>\n      <td>m</td>\n      <td>2004-03-29</td>\n      <td>58</td>\n      <td>floor of mouth</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>N</td>\n      <td>DOD</td>\n      <td>...</td>\n      <td>9.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>5</td>\n      <td>3143</td>\n      <td>H</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3167</td>\n      <td>m</td>\n      <td>2004-09-30</td>\n      <td>74</td>\n      <td>floor of mouth</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>Y</td>\n      <td>DOD</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>3167</td>\n      <td>H</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3169</td>\n      <td>m</td>\n      <td>2004-10-28</td>\n      <td>63</td>\n      <td>floor of mouth</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>N</td>\n      <td>died other</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2</td>\n      <td>3169</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3170</td>\n      <td>m</td>\n      <td>2004-11-22</td>\n      <td>59</td>\n      <td>other</td>\n      <td>4</td>\n      <td>2b</td>\n      <td>4a</td>\n      <td>Y</td>\n      <td>Died other</td>\n      <td>...</td>\n      <td>10.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>6</td>\n      <td>3170</td>\n      <td>H</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3230</td>\n      <td>m</td>\n      <td>2006-01-26</td>\n      <td>74</td>\n      <td>tongue</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Y</td>\n      <td>DOD</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n      <td>3230</td>\n      <td>H</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>3568</td>\n      <td>m</td>\n      <td>2010-03-04</td>\n      <td>74</td>\n      <td>floor of mouth</td>\n      <td>2</td>\n      <td>2c</td>\n      <td>4a</td>\n      <td>Y</td>\n      <td>died other</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>1</td>\n      <td>3568</td>\n      <td>H</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>3574</td>\n      <td>m</td>\n      <td>2010-04-15</td>\n      <td>51</td>\n      <td>tongue</td>\n      <td>2</td>\n      <td>2b</td>\n      <td>4a</td>\n      <td>N</td>\n      <td>died other</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>3</td>\n      <td>3574</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>3578</td>\n      <td>f</td>\n      <td>2010-04-26</td>\n      <td>44</td>\n      <td>other</td>\n      <td>4</td>\n      <td>2b</td>\n      <td>NaN</td>\n      <td>Y</td>\n      <td>alive</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>6</td>\n      <td>3578</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>3583</td>\n      <td>m</td>\n      <td>2010-06-07</td>\n      <td>56</td>\n      <td>tongue</td>\n      <td>3</td>\n      <td>2b</td>\n      <td>4a</td>\n      <td>Y</td>\n      <td>alive</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>6</td>\n      <td>3583</td>\n      <td>L</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>3585</td>\n      <td>m</td>\n      <td>2010-05-13</td>\n      <td>68</td>\n      <td>tongue</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>N</td>\n      <td>alive</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>6</td>\n      <td>3585</td>\n      <td>H</td>\n    </tr>\n  </tbody>\n</table>\n<p>102 rows × 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "new_metadata"
   ]
  },
  {
   "source": [
    "# Import data and preprocess"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "select = ['Tumour']\n",
    "\n",
    "total = pd.concat({\"{}-{}\".format(row[\"TMA\"], row[\"Core\"]):\n",
    "                   truncate(pd.read_pickle(row[\"hdf_loc\"]).query(f\"Tissue in {select}\").sample(frac = 1), start = 1000, end = 1800)\n",
    "                   for name, row in tqdm(list(metadata.iterrows()))}, names = [\"ID\"])\n",
    "\n",
    "wn_cols = total.columns\n",
    "#total = total.merge(new_metadata, left_on = \"Patient nu \", right_on = \"Patient Number\")\n",
    "#total = total.drop(np.nan, level = \"Overall Death \")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=79.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92ed1b8dc0df4fd59cad9d5f2500809b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "select = ['Tumour']\n",
    "\n",
    "total = pd.concat({\"{}-{}\".format(row[\"TMA\"], row[\"Core\"]):\n",
    "                   truncate(pd.read_pickle(row[\"hdf_loc\"]).sample(frac = 1), start = 1000, end = 1800)\n",
    "                   for name, row in tqdm(list(metadata.iterrows()))}, names = [\"ID\"])\n",
    "\n",
    "wn_cols = total.columns\n",
    "#total = total.merge(new_metadata, left_on = \"Patient nu \", right_on = \"Patient Number\")\n",
    "#total = total.drop(np.nan, level = \"Overall Death \")"
   ]
  },
  {
   "source": [
    "total = total.reset_index().merge(new_metadata, left_on = \"Patient nu \", right_on = \"Patient_Number\", how = 'inner')\n",
    "total = total.set_index(list(total.columns.difference(wn_cols))).dropna(axis = 1)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1year',\n",
       " '1yeardeath',\n",
       " '2year',\n",
       " '2years',\n",
       " '3year',\n",
       " '4year',\n",
       " '5year',\n",
       " 'ASMA',\n",
       " 'Annotation_loc',\n",
       " 'Case ID',\n",
       " 'Core_x',\n",
       " 'Core_y',\n",
       " 'Corrected Site AJCC/UICC',\n",
       " 'DODvsAlive',\n",
       " 'Date of death',\n",
       " 'Date of recurrence',\n",
       " 'Diagnosis Age',\n",
       " 'DiedvsAlive',\n",
       " 'ECS_x',\n",
       " 'ECS_y',\n",
       " 'ID',\n",
       " 'Last seen/reviewed',\n",
       " 'Last updated',\n",
       " 'N stage',\n",
       " 'OUTCOME',\n",
       " 'OUTCOME.updated by',\n",
       " 'Overall Death ',\n",
       " 'Pathogical Stage',\n",
       " 'Patient nu ',\n",
       " 'Patient_Number',\n",
       " 'Recurrence?',\n",
       " 'T stage',\n",
       " 'TMA',\n",
       " 'TMA site',\n",
       " 'Tissue',\n",
       " 'Years',\n",
       " 'advancing front type',\n",
       " 'age @ diag',\n",
       " 'date of diag',\n",
       " 'diff',\n",
       " 'envi_loc',\n",
       " 'gender_x',\n",
       " 'gender_y',\n",
       " 'lifespan',\n",
       " 'pN',\n",
       " 'pT',\n",
       " 'radiotherapy',\n",
       " 'recurrence',\n",
       " 'site',\n",
       " 'site of recurrence',\n",
       " 'stage',\n",
       " 'survival (months)',\n",
       " 1002.0,\n",
       " 1006.0,\n",
       " 1010.0,\n",
       " 1014.0,\n",
       " 1018.0,\n",
       " 1022.0,\n",
       " 1025.0,\n",
       " 1029.0,\n",
       " 1033.0,\n",
       " 1037.0,\n",
       " 1041.0,\n",
       " 1045.0,\n",
       " 1049.0,\n",
       " 1052.0,\n",
       " 1056.0,\n",
       " 1060.0,\n",
       " 1064.0,\n",
       " 1068.0,\n",
       " 1072.0,\n",
       " 1076.0,\n",
       " 1079.0,\n",
       " 1083.0,\n",
       " 1087.0,\n",
       " 1091.0,\n",
       " 1095.0,\n",
       " 1099.0,\n",
       " 1103.0,\n",
       " 1106.0,\n",
       " 1110.0,\n",
       " 1114.0,\n",
       " 1118.0,\n",
       " 1122.0,\n",
       " 1126.0,\n",
       " 1130.0,\n",
       " 1133.0,\n",
       " 1137.0,\n",
       " 1141.0,\n",
       " 1145.0,\n",
       " 1149.0,\n",
       " 1153.0,\n",
       " 1157.0,\n",
       " 1160.0,\n",
       " 1164.0,\n",
       " 1168.0,\n",
       " 1172.0,\n",
       " 1176.0,\n",
       " 1180.0,\n",
       " 1184.0,\n",
       " 1187.0,\n",
       " 1191.0,\n",
       " 1195.0,\n",
       " 1199.0,\n",
       " 1203.0,\n",
       " 1207.0,\n",
       " 1211.0,\n",
       " 1214.0,\n",
       " 1218.0,\n",
       " 1222.0,\n",
       " 1226.0,\n",
       " 1230.0,\n",
       " 1234.0,\n",
       " 1238.0,\n",
       " 1241.0,\n",
       " 1245.0,\n",
       " 1249.0,\n",
       " 1253.0,\n",
       " 1257.0,\n",
       " 1261.0,\n",
       " 1265.0,\n",
       " 1268.0,\n",
       " 1272.0,\n",
       " 1276.0,\n",
       " 1280.0,\n",
       " 1284.0,\n",
       " 1288.0,\n",
       " 1292.0,\n",
       " 1295.0,\n",
       " 1299.0,\n",
       " 1303.0,\n",
       " 1307.0,\n",
       " 1311.0,\n",
       " 1315.0,\n",
       " 1319.0,\n",
       " 1322.0,\n",
       " 1326.0,\n",
       " 1330.0,\n",
       " 1334.0,\n",
       " 1492.0,\n",
       " 1496.0,\n",
       " 1500.0,\n",
       " 1504.0,\n",
       " 1508.0,\n",
       " 1511.0,\n",
       " 1515.0,\n",
       " 1519.0,\n",
       " 1523.0,\n",
       " 1527.0,\n",
       " 1531.0,\n",
       " 1535.0,\n",
       " 1538.0,\n",
       " 1542.0,\n",
       " 1546.0,\n",
       " 1550.0,\n",
       " 1554.0,\n",
       " 1558.0,\n",
       " 1562.0,\n",
       " 1565.0,\n",
       " 1569.0,\n",
       " 1573.0,\n",
       " 1577.0,\n",
       " 1581.0,\n",
       " 1585.0,\n",
       " 1589.0,\n",
       " 1592.0,\n",
       " 1596.0,\n",
       " 1600.0,\n",
       " 1604.0]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "list(total.reset_index().columns)[:-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.reset_index().loc[:,['ID', 'Annotation_loc','envi_loc', 'Patient_Number']].drop_duplicates(subset='Patient_Number').to_excel('./metadata.xlsx')"
   ]
  },
  {
   "source": [
    "total = pd.concat({\"{}-{}\".format(row[\"TMA\"], row[\"Core\"]):\n",
    "                   truncate(pd.read_pickle(row[\"hdf_loc\"]).sample(frac = 0.1), start = 1000, end = 1800)\n",
    "                   for name, row in tqdm(list(metadata.iterrows()))}, names = [\"ID\"])\n",
    "\n",
    "wn_cols = total.columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "total.to_hdf(\"./Tumour_df_raw.hdf5\", key='Data')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.sample(5).T.plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, figsize = (6,4), sharex = True)\n",
    "\n",
    "total.groupby(\"1yeardeath\").mean().T.plot(ax = axes)\n",
    "axes.set_xlabel(\"Wavenumber $(cm^{-1})$\")\n",
    "axes.set_ylabel(\"Absorbance (a.u)\")\n",
    "#fig.savefig(os.getcwd()+ \"/Plots/Mean_ECS_Spectra.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Kaplan Meier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats = total.groupby('Patient_Number').sample(1).reset_index().iloc[:,:-169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration  = pats['survival (months)']\n",
    "death_obs = np.array([1 if i!='Died' else 0 for i in pats['DiedvsAlive']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "group = 'ASMA'\n",
    "\n",
    "for g in pats[group].dropna().unique():\n",
    "\n",
    "    kmf.fit(duration[(pats[group]==g)], death_obs[(pats[group]==g)], label=g)\n",
    "    kmf.plot(ax=ax, ci_show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "class PLSRegressionWrapper(PLSRegression):\n",
    "\n",
    "    def transform(self, X):\n",
    "        return super().transform(X)\n",
    "\n",
    "    def fit_transform(self, X, Y):\n",
    "        return self.fit(X,Y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipe = Pipeline([\n",
    "(\"Normalise spectra\", FunctionTransformer(robust_scale, kw_args = {\"axis\": 1})),\n",
    "#(\"Normalise spectra\", FunctionTransformer(normalize, kw_args = {\"axis\": 1})),\n",
    "#(\"Normalise spectra\", FunctionTransformer(minmax_scale, kw_args = {\"axis\": 1})),\n",
    "(\"Scaler\", MinMaxScaler()),\n",
    "#(\"Scaler\", StandardScaler()),\n",
    "(\"PCA\", PCA(0.99)),\n",
    "#('PLS', PLSRegressionWrapper(n_components=10))\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"OneHot\", OneHotEncoder())\n",
    "])\n",
    "\n",
    "in_df = total.reset_index(['ASMA']).dropna().sample(1000)\n",
    "#in_df = total.sample(10000)\n",
    "\n",
    "in_df.columns = [str(col) for col in in_df.columns]\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (numeric_pipe,     make_column_selector(dtype_include=np.number)),\n",
    "    (categorical_pipe, make_column_selector(dtype_include=object))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = pd.DataFrame(ct.fit_transform(in_df, in_df.reset_index()['1yeardeath']), index = in_df.index)"
   ]
  },
  {
   "source": [
    "# Bayesian Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano as th\n",
    "\n",
    "y = \"1yeardeath\"\n",
    "\n",
    "train_pat, test_pat = train_test_split(pats.reset_index()['Patient_Number'], test_size=0.33, random_state=np.random.randint(0,100))\n",
    "\n",
    "\n",
    "train_data = preproc.query(f\"Patient_Number in {list(train_pat)}\")\n",
    "test_data = preproc.query(f\"Patient_Number in {list(test_pat)}\")\n",
    "\n",
    "X_train = train_data\n",
    "X_test = test_data\n",
    "\n",
    "y_train, y_test = train_data.reset_index()[f\"{y}\"], test_data.reset_index()[f\"{y}\"]\n",
    "\n",
    "print(train_data.reset_index()[y].value_counts())\n",
    "print(test_data.reset_index()[y].value_counts())"
   ]
  },
  {
   "source": [
    "ax=sns.pairplot(preproc.reset_index(['1yeardeath']).sample(100), hue='1yeardeath')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncon = 13\n",
    "ncat = 3\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "\n",
    "    data_ = pm.Data('Pred', X_train.T)\n",
    "\n",
    "    ɛ = pm.HalfNormal('ɛ', sd=1, shape = (ncat+ncon+1,1))\n",
    " \n",
    "    # Continuous variables for each PC\n",
    "    β1 = pm.Normal(\"β1\", mu=0, sigma=1, shape = (ncon+1,1))\n",
    "\n",
    "    # Categorical variables \n",
    "    β2 = pm.Categorical(\"β2\", [1/ncat for _ in range(1, ncat+1)], shape=(ncat,1))\n",
    "\n",
    "    # β.T + ɛ\n",
    "    z = pm.math.dot(pm.math.concatenate([(β1[1:]+ɛ[1:-ncat]), (β2 + ɛ[-ncat:])]).T, data_)\n",
    "\n",
    "    # Probability of parameter P given the data\n",
    "    p = pm.Deterministic('P', pm.math.sigmoid(z + (β1[0] + ɛ[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "\n",
    "    obs = np.array([1.0 if el else 0.0 for el in y_train])\n",
    "\n",
    "    observed = pm.Bernoulli(\"p\", p, observed=obs.T)\n",
    "    start=pm.find_MAP()\n",
    "\n",
    "    #trace = pm.sample(10000, tune=100, start=start, step=step)\n",
    "    trace = pm.sample(1000, tune=3000, start=start, init=\"adapt_diag\", cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_graph.model_to_graphviz(logistic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_mean(array):\n",
    "\n",
    "    hist, edges = np.histogram(array)\n",
    "    hist = hist/np.sum(hist)\n",
    "\n",
    "    centres = (edges[:-1] + edges[1:])/2\n",
    "\n",
    "    return np.sum(hist*centres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"P\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [25,2,8]:\n",
    "    \n",
    "    plt.hist(trace[\"P\"].squeeze()[:,i], histtype='step', density=True, label=f'Sample {i}')\n",
    "    plt.axvline(trace[\"P\"].squeeze()[:,i].mean(), ls='--', c='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Prediction score')\n",
    "plt.ylabel('Likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [9,4,8]:\n",
    "    \n",
    "    plt.hist(trace[\"β1\"].squeeze()[:,i], histtype='step', density=True, label=f'β{i-1}')\n",
    "    #plt.axvline(trace[\"β1\"].squeeze()[:,i].mean(), ls='--', c='r')\n",
    "    plt.axvline(lr.coef_[:,i-1], ls='--', c='r')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.ylabel('Likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = pd.DataFrame(trace['P'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_U = np.mean(trace['P'], axis=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_w = p_df.apply(exp_mean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (18,5))\n",
    "\n",
    "preds = [1 if el > 0.5 else 0 for el in y_score_U]\n",
    "\n",
    "cm = confusion_matrix(obs, preds)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot(ax=ax1)\n",
    "\n",
    "fpr_w, tpr_w, thresh_w = roc_curve(obs, y_score_w);\n",
    "\n",
    "print(\"Weighted: {}\".format(roc_auc_score(obs, y_score_U)))\n",
    "print(\"Unweighted: {}\".format(roc_auc_score(obs, y_score_U.flatten())))\n",
    "\n",
    "RocCurveDisplay(fpr=fpr_w, tpr=tpr_w).plot(label='weighted', ax=ax2)\n",
    "#ax2.plot(fpr_u, tpr_u, label='unweighted')\n",
    "ax2.legend();\n",
    "ax2.set_title(\"ROC Curve\")\n",
    "ax2.plot([0,1],[0,1], ls='--')\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(obs, y_score_U)\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=ax3)\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    # update values of predictors:\n",
    "    pm.set_data({\"Pred\": X_test.T})\n",
    "    # use the updated values and predict outcomes and probabilities:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[\"P\"])\n",
    "\n",
    "    model_preds = posterior_predictive[\"P\"]\n",
    "\n",
    "t_df = pd.DataFrame(model_preds.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "y_preds = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "all = []    \n",
    "\n",
    "thresholds = np.linspace(0,1,20)\n",
    "\n",
    "for thresh in thresholds:\n",
    "\n",
    "    preds = [1 if el > thresh else 0 for el in y_preds]\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm.flatten()\n",
    "\n",
    "    sens, spec = tp/(tp+fn), tn/(tn+fp)\n",
    "\n",
    "    #print(f\"Specificity: {spec}\")\n",
    "    #print(f\"Sensitivity: {sens}\")\n",
    "\n",
    "    all.append(sens+spec)\n",
    "\n",
    "op = thresholds[np.argmax(all)]\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (18,5))\n",
    "\n",
    "#y_preds = t_df.mean(axis=0)\n",
    "\n",
    "preds = [1 if el > op else 0 for el in y_preds]\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot(ax=ax1)\n",
    "ax1.set_title(\"Confusion Matrix\")\n",
    "fpr_w, tpr_w, thresh_w = roc_curve(y_test, y_preds);\n",
    "\n",
    "print(\"AUC: {:.2f}\".format(roc_auc_score(y_test, y_preds)))\n",
    "#print(\"Unweighted: {}\".format(roc_auc_score(y_test, y_preds.flatten())))\n",
    "\n",
    "RocCurveDisplay(fpr=fpr_w, tpr=tpr_w).plot(label='weighted', ax=ax2)\n",
    "#ax2.plot(fpr_u, tpr_u, label='unweighted')\n",
    "ax2.legend();\n",
    "ax2.set_title(\"ROC Curve\")\n",
    "ax2.plot([0,1],[0,1], ls='--')\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_preds)\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=ax3)\n",
    "ax3.legend()\n",
    "ax3.set_title(\"Precision Recall Curve\")\n",
    "\n",
    "tn, fp, fn, tp = cm.flatten()\n",
    "\n",
    "print(f\"Specificity: {tn/(tn+fp):.2f}\")\n",
    "print(f\"Sensitivity: {tp/(tp+fn):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []    \n",
    "\n",
    "thresholds = np.linspace(0,1,20)\n",
    "\n",
    "for thresh in thresholds:\n",
    "\n",
    "    preds = [1 if el > thresh else 0 for el in y_preds]\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm.flatten()\n",
    "\n",
    "    sens, spec = tp/(tp+fn), tn/(tn+fp)\n",
    "\n",
    "    #print(f\"Specificity: {spec}\")\n",
    "    #print(f\"Sensitivity: {sens}\")\n",
    "\n",
    "    all.append(sens+spec)\n",
    "\n",
    "op = thresholds[np.argmax(all)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_max(row):\n",
    "\n",
    "    hist, edges = np.histogram(row, bins=np.linspace(0,1,100))\n",
    "\n",
    "    centres = (edges[:-1] + edges[1:])/2\n",
    "\n",
    "    return centres[np.argmax(hist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum(row):\n",
    "\n",
    "    hist, edges = np.histogram(row, density=True, bins=np.linspace(0,1,100))\n",
    "\n",
    "    centres = (edges[:-1] + edges[1:])/2\n",
    "\n",
    "    return np.where(centres<0.5, hist, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (18,5))\n",
    "\n",
    "y_preds = t_df.apply(row_max, axis=0)\n",
    "#y_preds = t_df.mean(axis=0)\n",
    "\n",
    "\n",
    "all = []    \n",
    "\n",
    "thresholds = np.linspace(0,1,20)\n",
    "\n",
    "for thresh in thresholds:\n",
    "\n",
    "    preds = [1 if el > thresh else 0 for el in y_preds]\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm.flatten()\n",
    "\n",
    "    sens, spec = tp/(tp+fn), tn/(tn+fp)\n",
    "\n",
    "    #print(f\"Specificity: {spec}\")\n",
    "    #print(f\"Sensitivity: {sens}\")\n",
    "\n",
    "    all.append(sens+spec)\n",
    "\n",
    "op = thresholds[np.argmax(all)]\n",
    "\n",
    "\n",
    "preds = [1 if el > op else 0 for el in y_preds]\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot(ax=ax1)\n",
    "ax1.set_title(\"Confusion Matrix\")\n",
    "fpr_w, tpr_w, thresh_w = roc_curve(y_test, y_preds);\n",
    "\n",
    "print(\"AUC: {:.2f}\".format(roc_auc_score(y_test, y_preds)))\n",
    "#print(\"Unweighted: {}\".format(roc_auc_score(y_test, y_preds.flatten())))\n",
    "\n",
    "RocCurveDisplay(fpr=fpr_w, tpr=tpr_w).plot(label='weighted', ax=ax2)\n",
    "#ax2.plot(fpr_u, tpr_u, label='unweighted')\n",
    "ax2.legend();\n",
    "ax2.set_title(\"ROC Curve\")\n",
    "ax2.plot([0,1],[0,1], ls='--')\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_preds)\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=ax3)\n",
    "ax3.legend()\n",
    "ax3.set_title(\"Precision Recall Curve\")\n",
    "\n",
    "tn, fp, fn, tp = cm.flatten()\n",
    "\n",
    "print(f\"Specificity: {tn/(tn+fp):.2f}\")\n",
    "print(f\"Sensitivity: {tp/(tp+fn):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "pats = total.groupby('Patient_Number').sample(1).reset_index().iloc[:,:-169]\n",
    "\n",
    "duration  = pats['survival (months)']\n",
    "death_obs = np.array([1 if i!='Died' else 0 for i in pats['DiedvsAlive']])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "group = 'ASMA'\n",
    "\n",
    "for g in pats[group].dropna().unique():\n",
    "\n",
    "    kmf.fit(duration[(pats[group]==g)], death_obs[(pats[group]==g)], label=g)\n",
    "    kmf.plot(ax=ax, ci_show=False)"
   ]
  }
 ]
}